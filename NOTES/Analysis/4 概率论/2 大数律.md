# 弱大数律

## 约束方差

我们通过对方差的运算和 Chebyshev 不等式快速证明一个较弱的版本

称随机变量类 $\{X_\alpha,\alpha\in I\}$ 无关, 如果 $\forall_{i\neq j},\ \mathbb E(X_iX_j)=\mathbb E X_i\mathbb EX_j$, 我们先从几个引理出发:

1. **无关则方差有限可加**: 若随机变量 $X_1,\cdots,X_n$ 无关且满足 $\mathbb E(X_{1}^2)<\infty$, 则
$$var(X_{1}+\cdots+X_{n})=var(X_{1})+\cdots+var(X_{n})$$
2. $var(cY)=c^2var(Y)$ 
3. $p>0$, $\mathbb E|Z_{n}|^p\to 0\Rightarrow Z_{n}\xrightarrow P 0$

这里其实能看出一点端倪, 在操作 $\frac{S_{n}}{n}$ 时, 分子的加法操作提供了 $n$ 级别的增长, 但除法提供了 $n^2$ 级别的下降

`BEGINPROOF`
(1):
$$\begin{align}
var\left( \sum X_{i} \right)&=\mathbb E\left( \left(\sum X_{i}\right)^2 \right)-\mathbb E\left( \sum X_{i} \right)^2 \\
&=\sum(\mathbb E (X_{i}^2)-\mathbb E(X_{i})^2)+\sum (\mathbb E(X_{i}X_{j})-\mathbb E(X_{i})\mathbb E(X_{j})) \\
&=\sum var(X_{i}) 
\end{align}$$
(3):
$$\forall\varepsilon>0, \mathbb P(|X_{n}|>\varepsilon)=\mathbb P(|X_{n}|^p>\varepsilon^p)< \frac{{\mathbb E(X_{n})}}{\varepsilon}\to 0$$
`ENDPROOF`

> [!thm] 
> 对无关随机变量列 $X_1,X_2,\cdots$ 满足 $EX_i=\mu$ 且 $var(X_i)\leq C<\infty$, 记 $S_n=\sum_{i=1}^nX_{n}$, 则
> $$\frac{S_{n}}{n}\xrightarrow{P\text{ or }L^2}\mu$$

`BEGINPROOF`
$L^2$:
$$\mathbb E\left( \frac{S_{n}}{n}-\mu \right)^2=var\left( \frac{S_{n}}{n} \right)=\frac{\sum var(X_{n})}{n^2}\leq \frac{C}{n}\to 0$$
$P$: $\forall \varepsilon>0$,
$$\mathbb P\left(\left| \frac{S_{n}}{n}-\mu\right|>\varepsilon \right)=\mathbb P\left(\left(\frac{S_{n}}{n}-\mu\right)^2>\varepsilon^2 \right)< \frac{\mathbb{E}\left( \frac{S_{n}}{n}-\mu \right)^2}{\varepsilon^2}\to0$$
`ENDPROOF`

我们发现最后的逼近 $1/n\to 0$ 还有加强的空间

> [!thm]
> $\mu_{n}=\mathbb ES_{n}$, $\sigma_n^2=var(S_n)$, 若 $\sigma_n/b_n\to 0$ 则
> $$\frac{{S_{n}-\mu_{n}}}{b_{n}}\xrightarrow{P}0$$

证明是显然的, 注意这里我们实质上关注了一个这样的情形

$$\begin{align}
S_{1}&=X_{11} \\
S_{2}&=X_{21}+X_{22} \\
S_{3}&=X_{31}+X_{32}+X_{33} \\
&\cdots
\end{align}$$

这也是大数律通常的推广方向, 例如 [[杂题荟萃#^cc095c]] 

## 约束期望

但这东西到底还是鸡肋, 实质上我们能利用截断拿到一个不依赖方差的版本

> [!thm] 
> i.i.d $\{X_n\}\subset L^{1}$, $\mathbb EX_n=m<\infty$, 记 $S_n=\sum_{j=1,\cdots,n}X_{j}$, 则
> $$\frac{S_{n}}{n}\xrightarrow P m$$

我们称两个随机变量列 $\{X_{n}\}$ 和 $\{Y_n\}$ 等价, 如果
$$\sum_{n}\mathbb P[X_{n}\neq Y_{n}]<\infty$$
由 bc 引理, 自然容易知道 
$$\mathbb P[X_n\neq Y_n \text{ for inf many }n]=\mathbb P[X_{n}=Y_{n} \text{ except finite many }n]=0$$
由此推出一些简单的性质:
- $\sum_{n}(X_{n}-Y_{n})$ a.s. 收敛
- $\frac{1}{n}\sum_{i=1}^n(X_{i}-Y_{i})\xrightarrow {\text{a.s.}}0$
- $\frac{1}{n}\sum_{i=1}^nX_{i}\xrightarrow PX\iff\frac{1}{n}\sum_{i=1}^nY_{i}\xrightarrow PX$ - 这正是我们需要的

`BEGINPROOF`
我们希望对随机变量列 $X_n$ 截断后得到的 $Y_n$ 与 $X_n$ 等价, 在本定理情形下, 我们可以取
$$Y_{n}=X_{n}\mathbb 1_{\{|X_{n}|<n\}}$$
此时
$$\sum_{n}\mathbb P[X_{n}\neq Y_{n}]=\sum_{n}\mathbb P[|X_{n}|>n]=\sum_{n}\mathbb P[|X_{1}|>n]\underset{L^1}<\infty$$
最后一步
$$X_{1}\in L^1\implies \infty>\int_{\Omega}|X_{1}|\geq 2\sum_{n }n\mathbb P[n<|X_{1}|<n+1]$$
如此截断后记 $T_n=\sum_{i=1}^nY_i$ 只需证明
$$\frac{{T_{n}-\mathbb E[T_{n}]}}{n}\xrightarrow P 0$$
而这只需 $var(T_n)=o(n^2)$, 也就是要控制下面这个东西 (记 $Z$ 与 $Y_i$ 同分布):
$$\begin{align}
var(T_{n})=\sum_{i=1}^nvar(Y_{i})\leq \sum_{i=1}^n\mathbb E(Y_{i}^2)=\sum_{j=1}^n\mathbb E[Z^2\mathbb 1_{\{|Z|<j\}}]
\end{align}$$
最平凡的限制
$$\sum_{j=1}^n\mathbb E[Z^2\mathbb 1_{\{|Z|<j\}}]<\sum j^2\sim O(n^3)$$
但是这不够, 再加一点条件, 我们有一阶矩有限
$$\sum_{j=1}^n\mathbb E[Z^2\mathbb 1_{\{|Z|<j\}}]<\sum j\mathbb E[|Z|\mathbb 1_{|Z|\leq j}]\leq \sum j\mathbb E[|Z|]\sim O(n^2)$$
但这还是不够, 我们还得再精细一点, 取一整数列 
$$\{a_n\}:1\leq a_n\leq n, a_n\to\infty, a_n\sim o(n)$$
然后计算
$$\begin{align}
&\sum_{j=1}^n \mathbb E[Z^2\mathbb 1_{\{|Z|\leq j\}}]=\sum_{j\leq a_{n}}+\sum_{a_{n<}j\leq n} \\
=&\sum_{j\leq a_{n}}\mathbb E[Z^2\mathbb 1_{\{|Z|\leq j\}}]+\sum_{a_{n}<j\leq n}\mathbb E[Z^2\mathbb 1_{\{|Z|\leq a_{n}\}}]+\sum_{a_{n}<j\leq n}\mathbb E[Z^2\mathbb 1_{\{a_{n}<|Z|\leq j\}}] \\
\leq&\underbrace{\sum_{j\leq a_{n}}a_{n}\mathbb E[|Z|]+\sum_{a_{n}<j\leq n}a_{n}\mathbb E[|Z|]}_{\sim O(1)na_{n}\sim o(n^2)}+ \underbrace{\sum_{a_{n}<j\leq n}n\mathbb E[|Z|\mathbb 1_{\{a_{n}<|Z|\leq j\}}]}_{O(1)n^2 \underbrace{\mathbb E[|Z|\mathbb 1_{\{|Z|>a_{n}\}}]}_{{o(1)}}}
\end{align}$$
`ENDPROOF`
## 强版本

我们甚至可以拿到一个不约束一阶矩的版本, 例如

> [!example]
> 取 iid $\{X_n\}$ 同分布于
> $$\mathbb P[Z=n]=\mathbb P[Z=-n]=\frac{c}{n^2\log n}\quad n=2,3,\cdots$$

^2fd1c9

#todo 

# Kolmogorov 三极限定理

这是对截断法的进一步研究, 某种把技术细节塞进定理里的好事

> [!thm] Kolmogorv 三极限定理
> 对独立随机变量列 $\{X_n\}$, 做截断
> $$Y_{n}=X_{n}\mathbb 1_{\{|X_{n}|<A\}}$$
> 则 $\sum_n X_n$ a.s. 收敛当且仅当以下三个极限收敛:
> $$\sum_{n}\mathbb P[|X_{n}|>A],\qquad \sum_{n}\mathbb E[Y_{n}],\qquad \sum_{n}var(Y_{n}).$$

`BEGINPROOF`
**充分性的证明:**
$$\sum_{n}\mathbb P[|X_{n}|>A]\geq\sum_{n}\mathbb P[X_{n}\neq Y_{n}]<\infty$$
这就是说 $X_n$ 和 $Y_n$ 等价, 所以只需说明级数 $\sum_n Y_n$ a.s. 收敛, 而由第二个级数, 只需说明 $\sum_n(Y_n-\mathbb E[Y_n])$ 收敛, 假设 $T_n=Y_n-\mathbb E Y_n$, 我们现在重述目前的问题:

对独立随机变量列 $T_n$, 若有 $\mathbb E[T_n]=0$, $\sum_n\mathbb E[T_n^2]$ 收敛, 往证 $\sum_nT_n$ 收敛, 我们先搞个不等式出来

> [!lem]
> 对独立随机变量列 $\{X_n\}$, 若有 $\mathbb E[X_n]=0$, $\mathbb E[X_n^2]<\infty$, 取 $S_n=\sum_{i=1}^nX_i$, 则
> $$\mathbb P[\max_{1\leq j\leq n}|S_{j}|\geq\varepsilon]\leq \frac{\mathbb{E}[S_{n}^2]}{\varepsilon^2}$$

`BEGINPROOF`
按 $k=\min_{j}|S_{j}|\geq\varepsilon$ 分类求和, 记事件
$$\begin{align}
\Lambda&:=\{\max_{1\leq j\leq n}|S_{j}|\geq\varepsilon\} \\
\Lambda_{k}&:=\left\{\max_{1\leq j\leq k-1}\right|S_{j}|<\varepsilon,|S_{k}|\geq\varepsilon\}
\end{align}$$
$\Lambda_{k}$ 为 $S_n$ 在 $k$ 处首次突破 $\varepsilon$ 则 $\Lambda_k$ 两两互斥且
$$\begin{align}
\mathbb E[S_{n}^2\mathbb 1_{\Lambda}]&=\sum_{k=1}^n\mathbb E[S_{k}^2\mathbb 1_{\Lambda_{k}}] \\
&=\sum_{k=1}^n\mathbb E[S_{k}^2\mathbb 1_{\Lambda_{k}}+\underbrace{2S_{k}(S_{n}-S_{k})\mathbb 1_{\Lambda_{k}}}_{\qquad =0\qquad (1)}+\underbrace{(S_{n}-S_{k})^2\mathbb 1_{\Lambda_{k}}}_{放缩}] \\
&\geq \sum_{k=1}^n\mathbb E[S_{k}^2\mathbb 1_{\Lambda_{k}}]\geq \sum_{k=1}^n\varepsilon^2\mathbb P[\Lambda_{k}]=\varepsilon^2\mathbb P[\Lambda]
\end{align}$$
补充证明 (1): $S_k\mathbb 1_{\Lambda_k}$ 与 $S_n-S_k=\sum_{i=k+1}^nX_{i}$ 独立, 从而
$$\mathbb E[S_{k}(S_{n}-S_{k})\mathbb 1_{\Lambda_{k}}]=\mathbb E[S_{k}\mathbb 1_{\Lambda_{k}}]\mathbb E[(S_{n}-S_{k})]=0$$
`ENDPROOF`


现在最核心的部分已经解决了, 还差一些抽象废话, 我们考虑证明上面这个东西是个Cauchy 列, $\forall\varepsilon>0$, $m,n \in\mathbb N^*$, 
$$\mathbb P\left[ \max_{m>j\geq n}\left |\sum_{i=n}^j T_{i}\right|\geq\varepsilon \right]\leq \varepsilon^{-2}\sum_{i=n}^j var(T_{i})$$
令 $m\to \infty$, 则
$$\mathbb P\left[ \max_{j\geq n}\left |\sum_{i\geq n}^j T_{i}\right|\geq\varepsilon \right]\leq \varepsilon^{-2}\sum_{i\geq n}var(T_{i})$$
从而 $\forall N\in\mathbb N^*$
$$\mathbb P\left[ \max_{m>n\geq N}\left |\sum_{i= n}^m T_{i}\right|\geq\epsilon \right]\leq \mathbb P\left[ \max_{j\geq N}\left |\sum_{i= N}^j T_{i}\right|\geq \frac{\epsilon}{2} \right]\leq 4\epsilon^{-2}\sum_{i\geq N}var(T_{i})$$
而由于 $\sum_{n}var(T_{n})$ 收敛, 两边对 $N$ 取极限
$$\lim_{ N \to \infty } \mathbb P\left[ \max_{m>n\geq N}\left |\sum_{i= n}^m T_{i}\right|\geq\epsilon \right]=0$$
而由于 $\{\max_{m>n\geq N}\left |\sum_{i= n}^m T_{i}\right|\geq\epsilon\}$ 单调递减, 由 fatou 引理
$$\mathbb P\left[ \lim_{ N \to \infty } \max_{m>n\geq N}\left |\sum_{i= n}^m T_{i}\right|\geq\epsilon \right]=0$$
令 $\varepsilon\to0$ 则
$$\mathbb P\left[ \lim_{ N \to \infty } \max_{m>n\geq N}\left |\sum_{i= n}^m T_{i}\right|=0 \right]=1$$
**再证明必要性:**

> [!lem]
> 对 a.s. 有界独立随机变量列 $\{X_n\}$: $\exists A>0,|X_n|\leq A \text{ a.s.}$, 记 $S_n=\sum_j=1^n X_j$, 则有
> $$\mathbb P\left [\max_{1\leq j\leq n}|S_{j}|\leq B\right]\leq \frac{(2B+A)^2}{var(S_{n})}$$

#todo 丑陋的技术细节

`ENDPROOF`
> [!example]
> 考虑 $X_n$ 为独立同分布于 $\mathbb P[X=1]=\mathbb P[X=-1]=1/2$ 的随机变量列, 则级数
> - $\sum_{n}X_{n}/n$ a.s. 收敛但不绝对收敛
> - $\sum_nX_n/\sqrt n$ a.s. 发散

利用三级数定理, 取 $Y_{n}= \frac{X_{n}}{n}$, $A=1$, 则 $\mathbb P[X_n/n|>1]=0$, $\mathbb E[Y_n]=0$, $var(Y_n)=1/n^2$, 从而显然.
对另一个级数, 取 $Y_n=X_n/\sqrt n$ 且 $A=1$, 可以验证其不 a.s 收敛, 同时由 $0$-$1$ 律可知其 a.s. 不收敛

# 强大数律

> [!thm] 强大数律
> iid $\{X_n\}$, 取 $S_n=\sum_{i=1}^nX_{i}$ 则
> - $\mathbb E[|X_1|]<\infty\implies \frac{S_{n}}{n}\xrightarrow{\text{a.s.}}\mathbb E[X_{1}]$
> - $\mathbb E[|X_{1}|]=\infty \implies\limsup_{n}\frac{|S_{n}|}{n}\xlongequal{\text{a.s.}}\infty$

^927e53

我们首先把上节介绍的三极限定理加工一下, 让它适合强大数律的情形

> [!lem] Kronecker's lem
> 对实数列 $\{x_n\}$ 和 $\{a_n\}$ 满足 $0<a_n\uparrow \infty$, 则
> $$\sum_{n} \frac{x_{n}}{a_{n}}\ 收敛\implies \frac{1}{a_{n}}\sum_{i=1}^nx_{i}\to0$$

^a5bdee

`BEGINPROOF`
假如 $x_n>0$, 该问题显然, 所以我们关键是要处理不绝对收敛的情形, 可以采取分部积分把正项级数 $a_n$ 移到主要位置上. 取 $b_n=\sum_{i=1}^nx_i/a_i$, 令 $a_0=b_0=0$ 则
$$\frac{1}{a_{n}}\sum_{i=1}^nx_{i}= \frac{1}{a_{n}}\sum_{j=1}^n a_{j}(b_{j}-b_{j-1})=b_{n}- \frac{1}{a_{n}}\sum_{j=0}^{n-1}(a_{j+1}-a_{j})b_{j}$$
这就是很经典的级数线性变换情形, 因为 $\frac{1}{a_{n}}\sum_{j=0}^{n-1}(a_{j+1}-a_{j})=1$, 于是
$$\begin{align}
\frac{1}{a_{n}}\sum_{j=0}^{n-1}(a_{j+1}-a_{j})b_{j}&\to b_{\infty} \\
b_{n}-\frac{1}{a_{n}}\sum_{j=0}^{n-1}(a_{j+1}-a_{j})b_{j}&\to 0
\end{align}$$
`ENDPROOF`

> [!lem]
> 对独立随机变量列 $\{X_n\}$ 满足 $\mathbb E[X_n]=0$. 若有数列 $0<a_n\uparrow \infty$. 若有偶的连续函数 $\varphi:\mathbb R\to \mathbb R^+$, 其增速满足
> $$\frac{\varphi(x)}{|x|}\uparrow,\qquad \frac{\varphi(x)}{x^2}\downarrow,\qquad \text{as }|x|\uparrow $$
> 并且有
> $$\sum_{n} \frac{\mathbb E[\varphi(X_{n})]}{\varphi(a_{n})}<\infty$$
> 则
> $$\sum_{n} \frac{X_{n}}{a_{n}}\quad \text{a.s. 收敛}\xRightarrow{\text{lem 10}} \frac{1}{a_{n}}\sum_{j=1}^n X_{j}\xrightarrow {\text{a.s.}}0$$

^dbe388

RMK: 这里对 $\varphi$ 的限制其实非常松, 例如可以取 $\varphi(x)=|x|^p$,  其中 $1\leq p\leq 2$

`BEGINPROOF`
用 $a_n$ 切一刀
$$Y_{n}=X_{n}\mathbb 1_{\{|X_{n}|\leq a_{n}\}}$$
然后套用三极限定理
$$\sum_{n}\mathbb P\left[  \frac{X_{n}}{a_{n}}\neq \frac{Y_{n}}{a_{n}} \right]=\sum_{n}\mathbb P [|X_{n}|>a_{n}]=\sum_{n}\mathbb P[\varphi(X_{n})>\varphi(a_{n})]\leq \sum_{n}\frac{\mathbb E[\varphi(X_{n})]}{\varphi(a_{n})}<\infty$$
由于 $\varphi(x)/|x|$ 单调增
$$\sum_{n} \frac{|\mathbb E[Y_{n}]|}{a_{n}}
=\sum_{n}\frac{|\mathbb E[X_{n}\mathbb 1_{\{|X_{n}|>a_{n}\}}]|}{a_{n}}\leq \sum_{n}\mathbb E\left[  \frac{|X_{n}|}{a_{n}}1_{\{|X_{n}|>a_{n}\}} \right]\leq \sum_{n}\mathbb E\left[  \frac{\varphi(X_{n})}{\varphi(a_{n})} \right]<\infty$$
由于 $\varphi(x)/x^2$ 单调减
$$\sum_{n}var\left(  \frac{Y_{n}}{a_{n}} \right)\leq \sum_{n}\mathbb E\left[  \frac{Y_{n}^2}{a_{n}^2} \right]=\sum_{n}\mathbb E\left[  \frac{X_{n}^2}{a_{n^2}}\mathbb 1_{\{|X_{n}|\leq a_{n}\}} \right]\leq \sum_{n}\mathbb E\left[  \frac{\varphi(X_{n})}{\varphi(a_{n})} \right]<\infty$$
`ENDPROOF`
所以这里实质上是拿一个极限套住了三个, 奇妙的想法

强大数律的证明

`BEGINPROOF`
**(1)**: 取 $Y_n=X_n\mathbb 1_{\{|X_n|\leq n\}}$ 则
$$\sum_{n}\mathbb P[X_{n}\neq Y_{n}]=\sum_{n}\mathbb P[|X_{1}|>n]\underset{L^1}{<}\infty$$
从而 $X_n$ 与 $Y_n$ 等价. 对 $Y_n$ 使用 [[#^dbe388]], 其中 $a_n=n$, $\varphi(x)=x^2$ 则
$$\begin{align}
\sum_{n} \frac{var(Y_{n})}{n^2}&\leq \sum_{n} \frac{\mathbb{E}[Y_{n}^2]}{n^2}=\sum_{n} \frac{1}{n^2}\mathbb E[ X^2\mathbb 1_{\{|X|\leq n\}}] \\
&=\sum_{n=1}^\infty \frac{1}{n^2}\sum_{j=1}^n\mathbb E[X^2\mathbb 1_{j-1<|X|\leq j}] \\
&=\sum_{j=1}^\infty\mathbb E[X^2\mathbb 1_{j-1<|X|\leq j}]\sum_{n\geq j} \frac{1}{n^2} \\
&=\sum_{j=1}^\infty\mathbb E[X^2\mathbb 1_{j-1<|X|\leq j}]\frac{O(1)}{j} \\
&\leq O(1)\sum_{j=1}^\infty\mathbb E[|X|\mathbb 1_{\{j-1<|X|\leq j\}}] =O(1)\mathbb E[|X|]<\infty
\end{align}$$
由 [[#^a5bdee]] 得到
$$\frac{1}{n}\sum_{j=1}^n[Y_{j}-\mathbb E[Y_{j}]]\xrightarrow {\text{a.s.}}0$$
从而
$$\lim_{ n \to \infty } \frac{S_{n}}{n}=\lim_{ n \to \infty } \frac{1}{n}\sum_{j=1}^n Y_{j}=\lim_{ n \to \infty } \frac{1}{n}\sum_{j=1}^n\mathbb E[Y_{j}]=\mathbb E[X]$$
关键是第二个等号把随机变量的运算变成了数的运算, 第三个等号就变成了常规的级数

**(2)**: 由于 $\mathbb E[|X_1|]=\infty$, $\forall A>0,\mathbb E[|X_1|/A]=\infty$, 则
$$\sum_{n}\mathbb P[|X_{n}|>An]=\infty$$
由 bc 引理 (注意这里用到了 $X_n$ 独立)
$$\mathbb P[|X_{n}|>An\ i.o.]=1$$
而这等价于
$$\mathbb P[ |S_{n}-S_{n-1}|=|X_{n}|>A_{n}\ i.o.]=\mathbb P\left[  |S_{n}|> \frac{An}{2}\ i.o. \right]=1$$
而由于 $A$ 是任意的, 这说明
$$\limsup_{n} \frac{|S_{n}|}{n}\xlongequal {\text{ a.s.}}\infty$$
`ENDPROOF`
在上面这个例子 [[#^2fd1c9]] 中, 虽然 $S_n/n$ 依概率趋向于 $0$, 但是其上下极限
$$\limsup_{n} \frac{S_{n}}{n}=\infty\qquad \liminf_{n} \frac{S_{n}}{n}=-\infty$$
这也是弱大数律相比强大数律的优越之处

# 一些类似情形的应用

> [!example]
> i.i.d. $\{X_n\}$ 满足 $\mathbb E[X_1]=0$, $var(X_1)=1$. 取 $S_n=\sum_{j=1}^nX_{j}$, 则对任意 $\varepsilon>0$
> $$\frac{Sn}{n^{1/2}(\log n)^{1/2+\varepsilon}}\xrightarrow{\text{a.s.}} 0$$

`BEGINPROOF`
应用强大数律可以得到
$$\frac{S_{n}}{n}\xrightarrow {\text{a.s.}}0$$
但我们想要一个更强的结果, 直接暴力使用三极限定理, 令 $a_n=n^{1/2}(\log n)^{1/2+\varepsilon}$, 做截断 $Y_n= \frac{X_{n}}{a_{n}}\mathbb 1_{\{X_{n}\leq a_{n}\}}$ 则
$$\sum_{n}\mathbb P[|X_{n}|>a_{n}]\leq \sum_{n} \frac{\mathbb{E}[X_{n}^2]}{a_{n}^2}=\sum_{n} \frac{1}{n(\log n)^{1+2\varepsilon}}<\infty$$
(级数敛散性的判别采用 [[幂级数#^760750]] )
$$\begin{align}
&\sum_{n}|\mathbb E[Y_{n}]|\leq \sum_{n}\mathbb E\left[  \frac{|X_{n}|}{a_{n}}\mathbb 1_{\{|X_{n}|\geq a_{n}\}} \right]\leq \sum_{n} \frac{\mathbb{E}[X_{n}^2]}{a_{n}^2}=\sum_{n} \frac{1}{a_{n}^2}<\infty \\
&\sum_{n}var(Y_{n})\leq \sum_{n} \frac{\mathbb{E}[X_{n}^2]}{a_{n}^2}=\sum_{n} \frac{1}{a_{n}^2}<\infty
\end{align}$$
由三极限定理 $\sum_n X_n/a_n$ a.s. 收敛, 再由 [[#^a5bdee]] 得到结果
`ENDPROOF`

> [!example]
> 对 i.i.d $X_n$ 满足 $\mathbb E[X_1]=0$ 且 $\mathbb E[|X_1|^p]<\infty$, 其中 $p\in(1,2)$, 则
> $$\frac{S_{n}}{n^{1/p}}\xrightarrow {\text{a.s.}}0$$

`BEGINPROOF`
同样的过程, 设 $a_n=n^{1/p}$, 则
$$\sum_{n}\mathbb P[X_{n}>a_{n}]=\sum_{n}\mathbb P[|X_{1}|^p>n]\leq\mathbb E[|X_{1}|^p]<\infty$$
我们尝试证明
$$\sum_{n}var\left( \frac{Y_{n}}{a_{n}} \right)<\infty,\qquad \frac{\sum\mathbb{E}[Y_{n}]}{a_{n}}\to 0$$
从而和 [[#^927e53]] 相同的逻辑即可得到结果. 假定 $Z$ 和 $X$ 具有相同的分布, 则
$$\begin{align}
\sum_{n\geq 1}var\left(  \frac{Y_{n}}{a_{n}} \right)&\leq \sum_{n\geq_{1}} \frac{1}{a_{n}^2}\mathbb E[Y_{n}^2]=\sum_{n} \frac{1}{a_{n}^2}\mathbb E[Z^2\mathbb 1_{\{|Z|<a_{n}\}}] \\
&=\sum_{n} \frac{1}{a_{n^2}}\sum_{j\geq n}\mathbb E[Z^2\mathbb 1_{\{a_{j}<|Z|\leq a_{j+1}\}}] \\
&=\sum_{j}\mathbb E[Z^2\mathbb 1_{\{a_{j}<|Z|\leq a_{j+1}\}}]\sum_{n\leq j} \frac{1}{a_{n}^2}
\end{align}$$
`ENDPROOF`

