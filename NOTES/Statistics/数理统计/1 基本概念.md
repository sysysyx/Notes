# 正态分布的抽样

## 三大分布

统计推断的核心内容: 抽样分布, 参数估计, 假设检验. 本章主要研究正态整体下各种统计量的分布:
$$X\sim N(a,\sigma^2),\quad \begin{cases}
f(x)= \frac{1}{2\pi \sigma}\exp\left\{ - \frac{(x-a)^2}{2\sigma^2} \right\} \\
\varphi_{k}(t)=\mathbb E(\exp(itX_{k}))=e^{iat- \frac{1}{2}\sigma^2t^2}
\end{cases}$$
由此容易推出正态分布样本均值的分布
$$\text{i.i.d.}X_{i}\sim N(a,\sigma^2)\implies \bar{X}\sim N(a, \sigma^2 / n)$$
方差的分布没有简单的表示, 但是可以推断

> [!proposition]
> 独立正态分布样本的均值和方差相互独立

`BEGINPROOF`



考虑正交阵 (其存在性由 Schmidt 正交化保证)
$$A=\begin{pmatrix}
\frac{1}{\sqrt{ n }}&\frac{1}{\sqrt{ n }}&\cdots&\frac{1}{\sqrt{ n }} \\
a_{21}&a_{22}&\cdots& a_{2n} \\
\vdots&\vdots&&\vdots \\
a_{n1}&a_{n 2}&\cdots&a_{nn}
\end{pmatrix}$$

记其 $Y=AX$ , 则由于 $X_{i}$ 为独立同分布的正态分布随机变量, $Y_i$ 也是正太分布, 并且
$$\mathbb E(Y_{i})=a\sum_{k=1}^na_{ik},\qquad var(Y_{i})=\sigma^2\sum a_{ik}^2=\sigma^2$$
并且由于正交性
$$\begin{align}
cov(Y_{i},Y_{j})&=\mathbb E[(Y_{i}-\mathbb EY_{i})(Y_{j}-\mathbb EY_{j})] \\
&=\sum_{k=1}^n\sum_{l=1}^na_{ik}a_{jl}\mathbb E[(X_{k}-a)(X_{l}-a)] \\
&=\sigma^2\sum_{k=1}^na_{ik}a_{jk}=0
\end{align}$$
从而 $Y_{i}$ 相互独立 (这里必须要正态分布才能推出), 再由
$$Y_{1}=\sqrt{ n }\bar{X}$$
而正交向量保持长度不变, 从而
$$\sum Y_{i}^2=\sum X_{n}^2\implies(n-1)S^2=\sum(X_{i}-\bar{X})^2=\sum X_{i}^2-n\bar{X}^2=\sum Y_{i}^2-Y_{1}^2=\sum_{i=2}^nY_{i}^2$$
这就是说 $\bar{X}$ 只与 $Y_{1}$ 有关, 而 $S^2$ 只与 $Y_{2}\cdots Y_{n}$ 有关.
`ENDPROOF`

为了表示方差的分布, 定义

> [!def] $\chi^2$ 分布
> 设 $\text{i.i.d. }X_{i}\sim N(0,1)$, 则称 $\xi=\sum_{i=1}^nX_{i}^2\sim \chi_{n}^2$, 其概率密度函数和特征函数形如
> $$g_{n}(x)=\begin{cases}
> \frac{x^{n/2-1}e^{-x/2}}{2^{n/2}\Gamma( n / 2)}&x>0 \\
> 0&x\leq0
> \end{cases}
> \qquad
> \varphi_{n}(t)=(1-2it)^{- n / 2 }
> $$
> 均值和方差为
> $$\mathbb E(\xi)=n,\qquad var(\xi)=2n $$

从而对独立正态分布样本的方差 $S_{n}=\sum_{i=1}^n(X_{i}-\bar{X}) / (n-1)$ 有
$$\frac{(n-1)S_{n}^2}{\sigma^2}\sim \chi_{n-1}^2$$

这里需要特殊关注系数 $n-1$, 这是因为 $\bar{X}$ 也是变量, 实际上的自由度只有 $n-1$

为了给出其它一些统计量的分布和性质, 再定义两种常用分布: $t$ 分布和 $F$ 分布

> [!def] $t$ 分布
> 设 $X\sim N(0,1),Y\sim \chi_{n}^2$ 且二者独立, 则称
> $$T= \frac{X}{\sqrt{ Y/n }}\sim t_{n}$$
> 其概率密度函数
> $$t_{n}(x)= \frac{\Gamma\left(  \frac{n+1}{2} \right)}{\Gamma\left(  \frac{n}{2} \right)\sqrt{ n\pi }}\left( 1+ \frac{x^2}{n}  \right)^{-(n+1)/2}$$

从而对上述例子
$$T= \frac{\sqrt{ n }(\bar{X}-a)}{S}\sim t_{n-1}$$

> [!def] $F$ 分布
> $X\sim \chi_{m}^2$, $Y\sim \chi_{n}^2$, 且 $X$, $Y$ 独立, 则称 $F= \frac{X / m}{Y / n}\sim F_{m,n}$, 其密度函数为
> $$f_{m,n}=\begin{cases}
> \frac{\Gamma\left(  \frac{m+n}{2} \right)}{\Gamma\left(  \frac{n}{2} \right)\Gamma\left(  \frac{m}{2} \right)}m^{m/2}n^{n/2}x^{m/2-1}(n+mx)^{-\frac{m+n}{2}}&x>0 \\
> 0&x\leq0
> \end{cases}$$

Properties:
(a) $$\begin{align}
\mathbb E(F)&= \frac{n}{n-2},\quad n>2 \\
\mathbb E(F^r)&=\left(  \frac{n}{m} \right)^r\frac{\Gamma\left(  \frac{n}{2}-r \right)\Gamma\left( \frac{m}{2}+r \right)}{\Gamma\left(  \frac{n}{2} \right)\Gamma\left( \frac{m}{2} \right)}
\end{align}$$
(b) 若记 $F(\alpha)=x:\mathbb P(F>x)=\alpha$ 则
$$F_{m,n}(1-\alpha)=1/F_{n,m}(\alpha)$$

## 次序统计量


# 统计模型简介


# 指数族

指数族是一类较好处理的统计模型, 如正态分布, 二项分布, Poisson 分布, 负二项分布, 指数分布, Gamma 分布. 本节介绍其定义和简单性质

> [!def] 指数族
> 设 $\mathcal F=\{f(x,\theta):\theta\in \Theta\}$ 定义在样本空间 $\mathcal X$ 上的分布族, 若其概率函数可表示为
> $$f(x,\theta)=C(\theta)\exp\left\{ Q^T(\theta)T(x) \right\}h(x),\quad C(\theta)>0,h(x)>0 $$
> 则称其为 $k$ 指数族, 并且若指数族能表示成
> $$f(x,\theta)=C^*(\theta)\exp\left\{ \theta^TT(x) \right\}h(x)$$
> 则称其为为指数族的自然形式, 此时集合
> $$\mathbb R^k\supset \Theta^*=\left\{ (\theta_{1},\cdots,\theta_{k}):\int_{\mathcal X}\exp\left\{ \theta^TT(x) \right\}h(x)\ \mathrm{d}x<\infty \right\}$$
> 称为自然参数空间

可以发现, $C^*(\theta)$ 是完全由 $T$, $h$ 决定的, 称为正交化常量

> [!thm]
> 自然参数空间为凸集

观察形式直接可得

> [!thm]
> 设自然参数空间有非空内点集 $\Theta_{0}$, 对任一实函数 $g(x)$ 使得积分
> $$G(\theta)=\int_{\mathcal X}g(x)\exp\left\{ \theta^TT(x) \right\}h(x)\ \mathrm{d}x$$
> 在 $\Theta_{0}$ 存在且有限, 则 $G$ 的任一阶偏导数在 $\Theta_{0}$ 内存在且可在积分号下求得

另一个有意思的事情是 $M_{\theta}^{T(X)}(u)= \frac{C(\theta)}{C(\theta+u)}$ 为 $T(X)$ 的矩生成函数:
$$\begin{align}
M_{\theta}^{T(X)}(u)&=\mathbb E[e^{u^TT(X)}] \\
&=\int e^{u^TT}e^{\theta^TT}C(\theta)h\ \mathrm{d}\mu \\
&= \frac{C(\theta)}{C(\theta+u)}\underbrace{\int e^{(\theta+u)^TT}C(\theta+u)h\ \mathrm{d}\mu }_{1}
\end{align}$$

这表明
$$\partial^IM_{\theta}^{T(X)}(0)=\mathbb E_{\theta}(T^I)$$
其中 $I$ 为多重指标
# 各种常用统计量

## 充分统计量

> [!def] 充分统计量
> 对于样本分布族 $\{f(\theta,x),\theta\in\Theta\}$, 令 $T=T(X)$ 为一统计量, 若在已知 $T$ 的条件下, 样本 $X$ 的条件分布与参数 $\theta$ 无关, 则称 $T(X)$ 为 $\theta$ 的充分统计量, 实际操作时也会用条件概率(离散情形) 或条件密度 (连续情形) 来代替.

这就是说充分统计量是确定参数 $\theta$ 的充分条件, 其判断方式也很直观

> [!thm] 因子分解定理
> 设样本 $X=(X_{1},\cdots,X_{n})$ 的概率函数 $f(\overrightarrow x,\theta)$ 依赖于参数 $\theta$, $T=T(X)$ 为一统计量, 则 $T=TX$ 为充分统计量的充要条件是 $f(x,\theta)$ 可以分解为
> $$f(x,\theta)=g(t(x),\theta)h(x)$$

给出一个便于理解但不严谨的瞎掰证明

`BEGINPROOF`
必要性显然, 考虑充分性, 令
$$\begin{align}
g_{\theta}(x):&=\mathbb P_{\theta}(T(X)=t)=\int_{T(x)=t}p_{\theta}(x)\ \mathrm{d}\mu \\
h(x):&=\mathbb P(X\mid T)= \frac{p_{\theta_{0}}(x)}{\int_{T(x)=T(x)}p_{\theta_{0}}(z)\ \mathrm{d}\mu (x)}&(\theta\ 无关)
\end{align}$$
从而
$$g_{\theta}(T(x))h(x)=\mathbb P(T(X)=T(x))\mathbb P_{\theta}(X=x\mid T(X)=T(x))=p_{\theta}(x)$$
`ENDPROOF`

下面是充分性的一个应用

> [!thm] Rao Blackwell
> 对于充分统计量 $T(X)$ 和 $g(\theta)$ 的估计 $\delta(X)$, 取 $\bar{\delta}(T(X))=\mathbb E[\delta(X)\mid T(X)]$, 若 $L(\theta;d)$ 是凸的, 则
> $$R(\theta;\bar{\delta})\leq R(\theta;\delta)\quad \forall\delta$$
> 并且若 $L$ 严格凸, 上式严格小于, 处分 $\delta\xlongequal{\text{a.s.}}\bar{\delta}$

^596d0c

`BEGINPROOF`
$$R(\theta;\bar{\delta})=\mathbb E_{\theta}[L(\theta;\mathbb E(\delta\mid T))]\leq\mathbb E_{\theta}[\mathbb E[L(\theta;\delta)\mid T]]=\mathbb E_{\theta}[L(\theta;\delta)]=R(\theta;\delta)$$
这个小于等于号就是 Jenson 不等式 $f凸\implies\mathbb E[f(X)]\geq f[\mathbb E[X]]$
`ENDPROOF`

在这个证明中, $\bar{\delta}$ 能成为统计量的一个必要条件就是 $T$ 为一充分统计量, 对一个估计做上述优化的操作称为 Rao-Blackwellization

## 极小充分统计量

对于分布族 $N(\theta,1)$, 我们至少能找到以下四个充分统计量
$$\begin{matrix}
X=(X_{1},\cdots,X_{n})& S(X)=(X_{(1)},\cdots,X_{(2)})) \\
\sum_{i}X_{i}&\bar{X}= \frac{1}{n}\sum_{i}X_{i}
\end{matrix}$$
甚至不讲道理的说, $X$ 本身是任何分布族的充分统计量, 但这显然不是我们想要的. 假如我们视充分统计量为某种无损的信息压缩, 那我们想要的实际上是那个最优的压缩方式, 既极小充分统计量

> [!def] 极小充分统计量
> 设 $T$ 是分布族 $\mathcal F$ 的充分统计量, 若对任一充分统计量 $S(X)$, 存在一个函数 $q_{s}(\cdot)$, 使得 $T=q_{s}\circ S$, 则称 $T(X)$ 为此分布的极小充分统计量

那么问题就变成了这种东西的存在性和唯一性, 存在性由 Zorn 引理保证, 唯一性参考下面的讨论

更进一步考虑上述的信息压缩, 我们压缩的实际上是 $X$ 所在的概率空间, 利用似然函数可以进一步说明

> [!thm]
> 对于 $\mathcal P=\{p_{\theta}\}$ 的充分统计量 $T(X)$, 若
> $$L(\theta;x)\propto_{\theta}L(\theta;y)\implies T(x)=T(y)$$
> 则 $T$ 为极小充分统计量

`BEGINPROOF`
反设对于充分统计量 $S$, 不存在 $f$ 使得 $f(S(x))=T(x)$, 则存在 $x,y$ 满足 $S(x)=S(y)$ 但 $T(x)\neq T(y)$, 则
$$L(\theta;x)=g_{\theta}(S(x))h(x)\propto_{\theta} g_{\theta}(S(y))h(y)=L(\theta;y)$$
出现矛盾
`ENDPROOF`
这就是说, 关键看变量的那部分影响了 $L$ 的形状或者 $l$ 的高低

> [!example] Laplace location family
> $$\begin{align}
> p_{\theta}(x)&= \frac{1}{2^n}e^{-|x-\theta|}\\
> p_{\theta}(x)&= \frac{1}{2^n}\exp\left( -\sum|x_{i}-\theta| \right) \\
> l(\theta;x)&=-\sum|x_{i}-\theta|-n\log_{2}
> \end{align}$$
> 其极小充分统计量是 $S(X)=(X_{(1)},\cdots,X_{(n)})$.

^ac3ca2
## 完全统计量

> [!def] 完全统计量
> 设 $\mathcal F$ 为一分布族, $\Theta$ 是其参数空间, 设 $T=T(X)$ 为一统计量, 若对任一满足条件
> $$\mathbb E_{\theta}(\varphi(T(X))),\quad \forall\theta\in\Theta $$
> 的 $\varphi$ 都有
> $$\mathbb P_{\theta}(\varphi(T(X))=0)=1\quad \forall\theta\in\Theta $$
> 则称 $T(X)$ 为一完全统计量

显然完全统计量的任一 (可测) 函数也是完全统计量

(a) 完全统计量可以被理解为极小性的某种加强 (\*), 并不是每个参数族都有完全统计量, 例如在 [[#^ac3ca2]] 中, 对极小充分统计量 $S(X)$ 取
$$f(S(X))=\underbrace{\operatorname{med}(X)}_{中位数}-\bar{X}\implies\mathbb E(f(S(X)))=0, \operatorname{med}(X)\neq_{\text{a.e.}} \bar{X}$$
(b) 实际上他的定义更像满秩, 假定 $T\sim t$, 则定义式可改写为
$$\mathbb E_{\theta}(\varphi(T(X)))=\int \varphi(x)t(x)\ \mathrm{d}\mu(x)=0\implies \varphi(x)\xlongequal{\text{a.e.}}0$$

> [!thm]
> 充分完全统计量一定是极小充分统计量

`BEGINPROOF`
假定存在极小充分统计量 $S(X)$, 充分完全统计量 $T(X)$, 取函数 $m$
$$m(S(X))=\mathbb E_{\theta}[T(X)\mid S(X)]$$
则由于 $S$ 的充分性, $m(S(X))$ 与 $\theta$ 无关. 取 $g(t)=t-m(f(t))$ 则
$$\mathbb E_{\theta}[g(T)]=\mathbb E_{\theta}[T]-\mathbb E_{\theta}[\mathbb E_{\theta}[T\mid S]]=0\qquad \forall\theta$$
由完备性 $T\xlongequal{\text{a.s.}}\mathbb E_{\theta}[T\mid S]\implies T\xlongequal{\text{a.s.}}S$
`ENDPROOF`

> [!example] 指数族中的统计量 $T$
> 对指数族
> $$p_{\theta}(x)=C(\theta)e^{\eta(\theta)^TT(x)}h(x)$$
> 考虑统计量 $T$ 的充分性与完全性, 由于似然函数
> $$\begin{align}
> L(\theta;x)\propto_{\theta}L(\theta;y)&\iff e^{\eta(\theta)^TT(x)}\propto_{\theta}e^{\eta(\theta)T(y)} \\
> &\iff \eta(\theta)^T(T(x)-T(y))=C(x,y)
> \end{align}$$
> 取 $\theta_{1}$, $\theta_{2}$ 对应式子相减得
> $$(\eta(\theta_{1})-\eta(\theta_{2}))^T(T(x)-T(y))=0$$
> 这就是说, 假如我们有 $\operatorname{span}\{\eta(\theta_{1})-\eta(\theta_{2})\}=\mathbb R^k$ 那么 $T$ 为极小充分统计量
> 事实上我们有
> - 当 $\eta(\Theta)$ 有内点时, $T$ 为充分完全统计量 ( #todo )
> - 当 $\operatorname{span}\{\eta(\theta_{1})-\eta(\theta_{2})\}=\mathbb R^k$ 时, $T$ 是极小充分统计量但未必是充分完全统计量
> - 当上述两个条件都不满足时, $T$ 未必是极小充分统计量

## 辅助统计量

> [!def] 辅助统计量
> 对于分布族 $\mathcal F=\{f_{\theta}:\theta\in \Theta\}$ 若统计量 $S$ 的分布与 $\theta$ 无关, 则称 $S$ 为辅助统计量

> [!thm] Basu
> 若 $T$ 为充分完全统计量, $V$ 为辅助统计量, 则对任意 $\theta\in \Theta$, $V(X)$ 与 $T(X)$ 独立

`BEGINPROOF`
试图证明 $\forall A,B,\theta$
$$\mathbb P_{\theta}(V\in A,T\in B)=\mathbb P_{\theta}(V\in A)\mathbb P(T\in B)$$
这等价于
$$\mathbb P_{\theta}(V\in A\mid T\in B)=\mathbb P_{\theta}(V\in A)$$
当 $\mathbb P_{\theta}(T\in B)>0$ 时, 取
$$q_{A}(T(X))=\mathbb P(V\in A\mid T(X)),\qquad p_{A}=\mathbb P(V\in A)$$
由于 $p_{A}$, $q_{A}$ 与 $\theta$ 无关
$$\mathbb E_{\theta}[q_{A}(T(X))-p_{A}]=0$$
从而由完备性立得
`ENDPROOF`

Basu 定理可以用于分辨统计量的独立性, 有时我们也会对一个模型的子模型使用这一定理

> [!exercise]
> $X_{1},\cdots,X_{n}\stackrel{\text{i.i.d}}{\sim}N(\mu,\sigma^2)$, 往证 $\bar{X}$ 与 $S^2$ 独立

`BEGINPROOF`
对任意 $\sigma^2$ 考虑模型
$$\mathcal Q_{\sigma^2}=\{N(\mu,\sigma^2)^n:\mu\in\mathbb R\}$$
写成指数族
$$\begin{align}
f_{\mu}(x)&=(2\pi \sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum(x_{i}-\mu)^2\right\} \\
&=\exp\left\{ -2\mu \sum x_{i}\right\}\frac{n\mu^2}{(2\pi \sigma^2)^{n/2}}e^{\sum x_{i}^2}
\end{align}$$
而 $\mu\in\mathbb R$ 自然参数空间有内点, 这说明 $\bar{X}$ 为 $\mathcal Q_{\sigma^2}$ 的一充分完全统计量 (注意不是原空间), 令一方面, 令 $Z_{i}=X_{i}-\mu\stackrel{\text{i.i.d}}\sim N(0,\sigma^2)$ (事实上这里 $Z$ 不是统计量, 它和 $\mu$ 有关), 则
$$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_{i}-\bar{X})^2=\frac{1}{n-1}\sum(Z_{i}-\bar{Z})$$
的分布与 $\theta$ 无关, 由 Basu 定理, 对任意 $\mu,\sigma^2$ $\bar{X}$ 和 $S^2$ 相互独立
`ENDPROOF`

# 点估计与 UMVUE

点估计既用一个统计量 $T(X)$ 估计参数 $g(\theta)$, 称一个估计为无偏估计, 如果
$$\mathbb E_{\theta}[T]=g(\theta)$$
此时称 $g(\theta)$ 可被无偏估计 (U-estimable)

另一个对点估计非常重要的参数是估计的方差 $\operatorname{var}(T)$, 他们合起来构成了对点估计优良程度的一种评价方法 - 均方误差函数
$$MSE(\delta)=\mathbb E_{\theta}\|\delta-\theta\|^2=var(\delta)+|\operatorname{Bisa}(\delta)|^2$$

称一个无偏估计 $\delta$ 是一致最小方差无偏估计 (uniform minimal variance unbaised estimator, UMVUE), 如果对任意的无偏估计 $\tilde{\delta}$, 有 $var(\delta)\leq var(\tilde{\delta})$ 或 $MSE(\delta)\leq MSE(\tilde{\delta})$, 

> [!thm] Lehmann-Scheffe
> 假定 $T(X)$ 为 $\mathcal P=\{f_{\theta}:\theta\in\Theta\}$ 的一充分完全统计量, 则对任意可无偏估计的 $g(\theta)$, 存在一个几乎处处意义下唯一的 UMVUE, 其形如 $\delta(T(X))$

`BEGINPROOF`
假定有一 $g(\theta)$ 的无偏估计 $\delta_{0}(X)$, 定义估计量 $\delta(T)=\mathbb E[\delta_{0}\mid T]$. 他是无偏的, 因为 
$$\mathbb E[\delta(T)]=\mathbb E_{\theta}[\mathbb E[\delta_{0}\mid T]]=\mathbb E[\delta_{0}]=g(\theta)$$
若 $\tilde{\delta}(T)$ 为另一无偏估计, 则 $\mathbb E[\delta(T)-\tilde{\delta}(T)]=0,\forall\theta$, 由完全性 $\delta(T)\xlongequal{\text{a.s.}}\tilde{\delta}(T)$ 从而由 [[#^596d0c]] 对任意无偏估计 $\delta^*(X)$
$$MSE(\theta,\underbrace{\mathbb E_{\theta}[\delta^*\mid T]}_{=\delta})\leq MSE(\theta;\delta^*)$$`ENDPROOF`
容易发现, 对任意的凸损失函数, 上述的定理都成立

综上, 假定我们已有一个充分完全统计量 $T$, 想要找到 UMVUE, 我们有两种选择
1. 找到一个形如 $\delta(T)$ 的无偏估计
2. 首先找到一个无偏估计 $\delta_{0}$ , 再计算 $\mathbb E[\delta_{0}\mid T]$


> [!example] 德国坦克问题
> 设想你正躲在战壕里观察德军的坦克序列号, 并希望从中了解德军共有多少辆坦克加入了战场, 假定德军的坦克编号排序为从 $1$ 开始的自然数顺序排序, 一个直观的想法是直接采用那个最大的序列号作为估计, 但是显然这一估计是不精确的, 下面将给出一个当前信息量下最优的估计:
> 对于 $X_{1},\cdots,X_{n}\stackrel{\text{i.i.d}}{\sim}U(0,\theta)$, 给出充分完全统计量统计量 $X_{(n)}$,
> 充分性显然, 验证完全性: 首先写出 $X_{n}$ 的分布
> $$\begin{align}
> f(t)=\binom{n}{1} \frac{1}{\theta}\times\left( \frac{t}{\theta} \right)^{n-1}=\frac{nt^{n-1}}{\theta^n}
> \end{align}$$
> 从而对任意 $g$ 满足 
> $$\mathbb E_{\theta}(g(T))=\frac{n}{\theta^n}\int_{0}^\theta t^{n-1}g(t)\ \mathrm{d}t=0,\quad \forall\theta$$
> 则 $g(t)\xlongequal{\text{a.s.}}0$ 这说明 $X_n$ 是完全的. 下面计算
> $$\mathbb E[X_{(n)}]=\frac{n}{\theta^n}\int_{0}^\theta t^n\ \mathrm{d}t=\frac{n\theta}{n+1}$$
> 所以给出 UMVUE 为 $\delta(X)=\frac{n+1}{n}X_{(n)}$
> 另一种找到这一估计的方法是, 首先拿出一个无偏估计 $\delta^*=2\bar{X}$ 再做修正
> $$\mathbb E[2\bar{X}\mid X_{(n)}]=2 \frac{ X_{(n)}+(n-1)\frac{X_{(n)}}{2} }{n}=\frac{n+1}{n}X_{(n)}$$
> 然而事实上, 假如我们不限于无偏估计, 实际上在所有形如 $cX_{(n)}$ 的估计中, $\frac{n+2}{n+1}X_{(n)}$ 在 MSE 上表现最好

> [!example]
> 考虑 Poission 分布族 $X_{1},\cdots,X_{n}\stackrel{\text{i.i.d}}{\sim}P(\theta)$ 其 p.m.f.
> $$p_{\theta }^{(1)}(x)=\frac{\theta^xe^{-\theta}}{x!},\quad x\in \mathbb N$$
> 则 $T(X)=\sum X_{i}\sim P(n\theta)$ 为一充分完全统计量, 其 p.m.f
> $$p_{\theta}^T(t)=\frac{(n\theta)^te^{-n\theta}}{t!}$$
> 尝试对 $\theta^2$ 进行无偏估计
> 首先尝试方法一: 拿 $\bar{X}^2$ 试一下, 由于 Jensen 不等式 $\mathbb E(\bar{X}^2)>\mathbb E(\bar{X})^2=\theta^2$, 行不通
> $$\begin{align}
> \delta(T)\ 无偏&\iff \sum\delta(t)p_{\theta}^T(t)=\theta^2&\forall\theta \\
&\iff \sum_{t=0}^\infty\delta(t)\frac{n^t\theta^t}{t!}=\theta^2e^{n\theta}=\sum_{k=0}^\infty\frac{n^k\theta^{k+2}}{k!}&\forall\theta
> \end{align}$$
>  从而有 $\delta(t)=\frac{n^{t-2}}{(t-2)!}\cdot\frac{t^2}{n^t}=\frac{t(t-1)}{n^2}$ 
>  方法二 (Rao-Blackwelliza): 首先随便找一个无偏估计 $\mathbb E_{\theta}[X_{1}X_{2}]=\theta^2$, 直接计算
>  $$\delta(T)=\mathbb E\left[ X_{1}X_{2}\mid \sum X_{i} \right]=\frac{T(T-1)}{n^2}$$

## 得分函数的微分恒等式和方差下界

假定 $\mathcal P$ 的分布关于 $\theta$ 具有共同的支撑集, 对数似然函数可以取 $l(\theta;x)=\log p_{\theta}(x)$, 此时定义**得分函数** (score function #xjb翻译 ) 为 $\nabla l(\theta,x)$, 此时由于对较小的 $\eta$ 有
$$p_{\theta+\eta}(x)=e^{l(\theta+\eta;x)}\approx p_{\theta}(x)e^{\eta^T\nabla l(\theta;x)}$$
这就是说, 局部上看, $\mathcal P$ 差不多是个指数族, 而 $\nabla l$ 差不多是个充分完全统计量, 而由于我们有
$$1=\int_{\mathcal X}e^{l(\theta,x)}\ \mathrm{d}\mu(x)\implies0= \frac{\partial}{\partial\theta_{j}}1=\int_{\mathcal X}\partial_{j}l(\theta;x)e^{l(\theta;x)}\ \mathrm{d}\mu(x)$$
这就是说
$$\mathbb E_{\theta}[\nabla l(\theta;X)]=0$$
对上边那个式子再微分一次就有
$$0=\int_{\mathcal X}\left( \frac{\partial^2l}{\partial\theta_{j}\partial\theta_{k}}+\frac{\partial l}{\partial\theta_{j}}\frac{\partial l}{\partial\theta_{k}} \right)e^{l(\theta;x)}=\mathbb E_{\theta}\left[  \frac{\partial^2}{\partial\theta_{j}\partial\theta_{k}}l(\theta;X) \right]+\mathbb E\left[ \frac{\partial}{\partial\theta_{j}}l(\theta;X)+ \frac{\partial}{\partial\theta_{k}}l(\theta;X) \right]$$
从而有
$$J(\theta):=\mathbb E_{\theta}[-\nabla^2l(\theta;X)]=var_{\theta}(\nabla l(\theta;X))$$
此处的 $J(\theta)$ 称为 $\theta$ 的 Fisher 信息量

把上面这一套再应用回估计中, 假定
$$g(\theta)=\mathbb E_{\theta}[\delta(X)]=\int_{\mathcal X}\delta(x)e^{l(\theta;x)}\ \mathrm{d}\mu(x)$$
则
$$\begin{align}
\nabla g(\theta)&=\int\delta \nabla le^l \\
&=\mathbb E_{\theta}[\delta(X)\nabla l(\theta;X)] \\
&=cov(\delta(X),\nabla l(\theta;X))
\end{align}$$
假定只有一个参数, 既 $\theta\in\mathbb R$, 则由 Cauchy-Schwarz ineq
$$var_{\theta}(\delta)var(l)\geq cov_{\theta}(\delta,l)^2$$
> [!thm] Cramer-Rao
> 对任意的 $g(\theta)$ 无偏估计 $\delta(X)$, 
> - $\forall x\in\mathcal X,\theta\in\Theta$, $\frac{\partial f(x\mid \theta)}{\partial\theta}$ 存在, 且在 $f$ 的支撑集上不恒为 $0$
> - $\frac{\partial}{\partial\theta}\int f(x\mid \theta)\ \mathrm{d}x=\int \frac{\partial}{\partial\theta}f(x\mid \theta)\ \mathrm{d}x$
> - $\frac{d}{d\theta}\mathbb E_{\theta}[W(X)]=\int \frac{\partial}{\partial\theta}[W(x)f(x\mid \theta)]\ dx$
> 若有 $\theta\in\mathbb R$ 则
> $$var_{\theta}(\delta)\geq \frac{g'(\theta)^2}{J(\theta)}$$
> 更普遍地, 对 $\theta\in\mathbb R^d$, $g(\theta)\in\mathbb R$
> $$var_{\theta}(\delta)\geq \nabla g(\theta)^TJ(\theta)^{-1}\nabla g(\theta)$$

事实上, 也可以先射箭后画靶子, 既指定 $g(\theta)=\mathbb E_{\theta}[\delta]$

但是在 Cramer-Rao 不等式中, 我们需要在积分号下求导, 若稍微放宽条件能否得到类似的结论呢, Hammersley-Chapman-Rabbins ineq 利用变差给出了一个更泛用的下界, 其核心想法是
$$\epsilon\ll 1\implies \frac{p_{\theta+\epsilon}(x)}{p_{\theta}(x)}-1\approx \epsilon^T\nabla l(\theta;x)$$
> [!thm] Hammersley-Chapman-Robbins
> 令 $\delta$ 为 $g(\theta)$ 的一个无偏估计, 假定对于某些 $\epsilon$ 有 $p_{\theta+ \epsilon}\ll p_{\theta}$ 则
> $$var_{\theta}(\delta)\geq \sup_{\epsilon}\frac{g(\theta+\epsilon)-g(\theta)}{\mathbb E_{\theta}\left[ \left(  \frac{p_{\theta+\epsilon}(X)}{p_{\theta}(X)}-1 \right)^2 \right]}$$

rmk: 
- 这里的 $\ll$ 不是远小于的意思, 而是连续性, 即 $p_{\theta}\to 0\implies p_{\theta+\epsilon}\to0$, 保证除法可行
- 此处若取 $\epsilon\to0$ 即得 Cramer-Rao 不等式, 但是取 sup 可以得到一个更好的上界

`BEGINPROOF`
注意到若有 $p_{\theta+\epsilon}\ll p_{\theta}$
$$\mathbb E_{\theta}\left[ \frac{p_{\theta+\epsilon}(x)}{p_{\theta}(x)} -1\right]=\int\left(  \frac{p_{\theta+\epsilon}}{p_{\theta}} -1 \right)p_{\theta}\ \mathrm{d}\mu=\int(p_{\theta+\epsilon}-p_{\theta})\ \mathrm{d}\mu=0$$
更进一步
$$\begin{align}
cov\left( \delta(X), \frac{p_{\theta+\epsilon}(X)}{p_{\theta}(X)}-1 \right)&=\int\delta p_{\theta+\epsilon}\ \mathrm{d}\mu-\int\delta p_{\theta}\ \mathrm{d}\mu \\
&=\mathbb E_{\theta+\epsilon}[\delta(X)]-\mathbb E_{\theta}[\delta(X)] \\
&=g(\theta+\epsilon)-g(\theta)
\end{align}$$
从而由 Cauchy-Schwarz 立得题式
`ENDPROOF`

## 估计效率

下一个问题是, Cramer-Rao 下界未必同时能取到, 我们将逼近这一下界的程度定义为估计的效率:
$$e_{\theta}(\delta)=\frac{\text{Cramer-Rao Lower Bound}}{var_{\theta}(\delta)}= \frac{g'(\theta)^2}{var_{\theta}(\delta)J(\theta)}\leq 1$$
可以发现另一个等价定义
$$e_{\theta}(\delta)=corr(\delta,l')^2=\frac{(\partial_{\theta}\mathbb E_{\theta}[\delta(X)])^2}{var_{\theta}(\delta)var(l'(\theta;X))}$$
#todo 没理解, corr 是笔误还是什么我不知道的函数呢

> [!example]
> 对于指数族
> $$p_{\theta}(x)=C(\theta)e^{\theta^TT(x)}h(x),\qquad l(\theta;x)=\theta^TT(x)+\log C(\theta)+\log h(x) $$
> 其评分函数
> $$\nabla l(\theta;x)=T(x)-\mathbb E_{\theta}[T(X)]$$
> (这里用了指数族的性质) 从而 Fisher 信息量为
> $$var_{\theta}(\nabla l(\theta;X))=var_{\theta}(T(X))=\nabla^2(-\log C(\theta))=\mathbb E_{\theta}[-\nabla^2l(\theta;X)]$$

# Bayes 估计

> [!example]
> 对二项分布 $X\sim B(n,\theta)$, 考虑两个估计量 $\delta_{0}(x)=\frac{x}{n}$ 和 $\delta_{1}(x)= \frac{x+3}{n+6}$ 后者倾向于估计值 $\frac{1}{2}$, 如何评价二者的估计精度?

过去我们只考虑无偏估计的好坏, 但我们可以直接优化 MSE 而不是局限于无偏估计, 事实上无偏估计有时候会给出一些很唐的结果

> [!example]
> 假定 $X\sim B(50,\theta)$, $g(\theta)=\mathbb P_{\theta}(X>25)$ 其 UMVUE 是 $\delta(X)=\mathbb 1_{\{X\geq25\}}$, 但是假如我们抽到了 $X=25$, 就会直接估计这一概率为 $1$
> 假定 $X\sim N_{d}(\theta,I_{d})$, 我们试图估计 $\|\theta\|_{2}^2$, 其 UMVUE 是 $\|X\|_{2}^2-d$, 但这个估计甚至能小于 $0$, 这显然不太合理

为解决这一问题, 我们提出 Bayes 估计, 假定给出损失函数 $L(\theta;\delta(X))$, 通常的风险函数为 $R(\theta;\delta)=\mathbb E_{\theta}[L(\theta;\delta(X))]$, 我们定义 Bayes 风险
$$R_{\text{Bayes}}(\Lambda;\delta)=\int_{\Omega}R(\theta;\delta)\ \mathrm{d}\Lambda(\theta)=\mathbb E[R(\Theta;\delta(X))]=\mathbb E[L(\Theta;\delta(X))]$$
这里 $\Theta\sim\Lambda$ 且 $X\mid \Theta=\theta\sim P_{\theta}$, 这里将 $R$ 依照某一给定的测度 $\Lambda$ 积分, 这里 $\Lambda$ 称为先验 (prior #xjb翻译 ), 我们暂且假定 $\Lambda(\Omega)=1$, 随后我们会允许 $\Lambda(\Omega)=\infty$, 称为不当先验

> [!def] Bayes 估计
> 若 $\delta(X)$ 最小化 $R_{\text{Bayes}}(\Lambda,\delta)$, 则称其为 Bayes 估计

事实上更加常用的定义是

> [!thm]
> $\Theta\sim\Lambda$, $X\mid _{\Theta=\theta}\sim P_{\theta}$, 若 $\forall\theta,d, L(\theta;d)\geq0$ 且 $\exists\delta_{0},R_{\text{Bayes}}(\Lambda;\delta_{0})<\infty$ 则
> $$\delta_\Lambda(x)\in \arg\min_{d}\mathbb E[L(\Theta;d)\mid X=x]\text{ for a.e.}x\iff \delta_{\Lambda}\text{ is Bayes}$$

`BEGINPROOF`
$\implies$: 对任意估计 $\delta$
$$\begin{align}
R_{\text{Bayes}}(\Lambda;\delta)&=\mathbb E[L(\Theta;\delta(X))] \\
&=\mathbb E[\mathbb E[L(\Theta;\delta(X))\mid X]] \\
&\geq \mathbb E[\mathbb E[L(\Theta;\delta_{\Lambda}(X))\mid X]] \\
&=R_{\text{Bayes}}(\Lambda;\delta_{\Lambda})
\end{align}$$
$\impliedby$: 反证, 设 $E_{x}(d):=\mathbb E[L(\Theta;d)\mid X=x]$, 取
$$\delta^*(x)=\begin{cases}
\delta_{\Lambda}(x)&\delta_{\Lambda}(x)\in\arg\min E_{x}(d) \\
\delta_{0}(x)&E_{x}(\delta_{0}(x))<E_{x}(\delta_\Lambda(x)) \\
d^*(x)&\text{otherwise} 
\end{cases}$$
其中 $d^*(x)$ 是使 $E_{x}(d^*(x))<E_{x}(\delta_{\Lambda}(x))$ 的点, 则
$$\begin{align}
E_{x}(\delta^*(X))&\leq_{\text{a.s.}} E_{x}(\delta_{0}(X)) \\
E_{x}(\delta^*(X))&\leq_{\text{a.s.}} E_{x}(\delta_{\Lambda}(X))
\end{align}$$
并且若后者在一个正测度集上取严格小于
$$R_{\text{Bayes}}(\Lambda,\delta^*)\leq R_{\text{Bayes}}(\delta_\Lambda(X))$$
产生矛盾
`ENDPROOF`

> [!proposition]
> 当我们取损失函数 $L(\theta;d)=(g(\theta)-d)^2$ 为平方误差, Bayes 估计即为
> $$\delta=\mathbb E[g(\Theta)\mid X]$$

`BEGINPROOF`
这是因为
$$R_{\text{Bayes}}(\Lambda,\delta)=\mathbb E[\mathbb E[(g(\Theta)-\delta(X))^2\mid X]]=\mathbb E[var(g(\Theta)\mid X)+(\mathbb E[g(\Theta)\mid X]-\delta(X))^2]$$
要最小化后面这个东西就必然要 $\delta(X)\xlongequal{\text{a.s.}}\mathbb E[g(\Theta)\mid X]$
`ENDPROOF`
> [!proposition]
> 若 $d\Lambda=\lambda dx$ 则称 $\lambda$ 为先验分布, 此时称 $\lambda(\theta\mid X)$ 为后验分布, $\delta_{\Lambda}(X)$ 最小化
> $$\mathbb E[L(\Theta;\delta_{\Lambda})\mid X=x]$$

## 共轭先验

> [!example] Beta-Binomial
> $X\sim B(n,\theta)=\theta^x(1-\theta)^{1-x}\binom nx$, 其先验分布 $\theta\sim Beta(\alpha,\beta)=\theta^{\alpha-1}(1-\theta)^{\beta-1}\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$
> 则后验分布
> $$\lambda(\theta\mid x)=\frac{p_{\theta}(x)\lambda(\theta)}{\int _{0}^1p_{v}(x)\lambda(v)\ \mathrm{d}v}\propto_{\theta}\theta^{x+\alpha-1}(1-\theta)^{\beta-x}\propto Beta(\alpha+x-1,\beta-x+1) $$
> 从而均方误差意义下
> $$\mathbb E[\Theta\mid X]= \frac{X+\alpha}{n+\alpha+\beta} $$

假如先验分布和后验分布来自同一分布族, 我们就称其为共轭先验 (显然这个定义并不很清晰, 其实就是个称呼而言), 一些常见的共轭先验

| 似然函数 | 先验分布(族) |
| ---- | ---- |
| 二项分布 | Beta 分布 |
| 正态分布 | 正态分布 |
| 泊松分布 | Gamma 分布 |

## 先验的种类

Bayes 估计要求我们凭空造出一个先验分布出来, 但问题是如何给出呢

- 直接先验, 平行检验: 我们可以从数据中获得先验, 若我们有对先验的广泛共识, 则更容易得到有意义的先验, 为此我们给出了以下两种 Bayes 估计
	- 多层 Bayes (Hierarchical Bayes)
	- 经验 Bayes (Empirical Bayes) #xjb翻译 
- 主观意愿: 统计的本质是大忽悠
- 找个好算的
- 客观先验: 假装这个先验不带主观判断

> [!example]
> 假定 $X_{i}\sim N(\theta,1)$, 应用平滑先验 $\lambda(\theta)\propto_{\theta}1$ (注意我们从来没要求先验是个概率分布), 我们仍然可以计算
> $$\lambda(\theta)p_{\theta}(x) \propto_{\theta} \exp\left\{ \theta \sum_{i}x_{i}-n\theta^2 / 2 \right\}\propto N\left( \bar{x}, \frac{1}{n} \right)$$
> 从而 Bayes 估计量为 $\bar{X}$, 

但是平滑先验有一个重要问题, 就是他在重参数化意义下不变

## 多层 Bayes

#todo 

## Markov Chain Monte Caelo, MCMC

> [!def] Markov 链
> 称一系列随机变量 $X^{(0)},X^{(1)},\cdots$ 是核为 $Q(y\mid x)$, 初始分布为 $\pi_{0}(x)$ 的 Markov 链, 是指其分布
> $$X^{(0)}\sim \pi_{0},\qquad X^{(n+1)}\mid X^{(0)},\cdots,X^{(t)}\sim Q(\cdot\mid X^{(t)})$$
> 换句话说, Markov 链上的元素每个影响下一个

