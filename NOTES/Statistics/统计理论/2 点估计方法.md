#todo 多层 Bayes 和经验 Bayes 那一段要找个好点的 ref 重写一下

# 点估计与 UMVUE

点估计既用一个统计量 $T(X)$ 估计参数 $g(\theta)$, 称一个估计为无偏估计, 如果
$$\mathbb E_{\theta}[T]=g(\theta)$$
此时称 $g(\theta)$ 可被无偏估计 (U-estimable)

另一个对点估计非常重要的参数是估计的方差 $\operatorname{var}(T)$, 他们合起来构成了对点估计优良程度的一种评价方法 - 均方误差函数
$$MSE(\delta)=\mathbb E_{\theta}\|\delta-\theta\|^2=var(\delta)+|\operatorname{Bisa}(\delta)|^2$$

称一个无偏估计 $\delta$ 是一致最小方差无偏估计 (uniform minimal variance unbaised estimator, UMVUE), 如果对任意的无偏估计 $\tilde{\delta}$, 有 $var(\delta)\leq var(\tilde{\delta})$ 或 $MSE(\delta)\leq MSE(\tilde{\delta})$, 

> [!thm] Lehmann-Scheffe
> 假定 $T(X)$ 为 $\mathcal P=\{f_{\theta}:\theta\in\Theta\}$ 的一充分完全统计量, 则对任意可无偏估计的 $g(\theta)$, 存在一个几乎处处意义下唯一的 UMVUE, 其形如 $\delta(T(X))$

`BEGINPROOF`
假定有一 $g(\theta)$ 的无偏估计 $\delta_{0}(X)$, 定义估计量 $\delta(T)=\mathbb E[\delta_{0}\mid T]$. 他是无偏的, 因为 
$$\mathbb E[\delta(T)]=\mathbb E_{\theta}[\mathbb E[\delta_{0}\mid T]]=\mathbb E[\delta_{0}]=g(\theta)$$
若 $\tilde{\delta}(T)$ 为另一无偏估计, 则 $\mathbb E[\delta(T)-\tilde{\delta}(T)]=0,\forall\theta$, 由完全性 $\delta(T)\xlongequal{\text{a.s.}}\tilde{\delta}(T)$ 从而由 [[#^596d0c]] 对任意无偏估计 $\delta^*(X)$
$$MSE(\theta,\underbrace{\mathbb E_{\theta}[\delta^*\mid T]}_{=\delta})\leq MSE(\theta;\delta^*)$$`ENDPROOF`
容易发现, 对任意的凸损失函数, 上述的定理都成立

综上, 假定我们已有一个充分完全统计量 $T$, 想要找到 UMVUE, 我们有两种选择
1. 找到一个形如 $\delta(T)$ 的无偏估计
2. 首先找到一个无偏估计 $\delta_{0}$ , 再计算 $\mathbb E[\delta_{0}\mid T]$


> [!example] 德国坦克问题
> 设想你正躲在战壕里观察德军的坦克序列号, 并希望从中了解德军共有多少辆坦克加入了战场, 假定德军的坦克编号排序为从 $1$ 开始的自然数顺序排序, 一个直观的想法是直接采用那个最大的序列号作为估计, 但是显然这一估计是不精确的, 下面将给出一个当前信息量下最优的估计:
> 对于 $X_{1},\cdots,X_{n}\stackrel{\text{i.i.d}}{\sim}U(0,\theta)$, 给出充分完全统计量统计量 $X_{(n)}$,
> 充分性显然, 验证完全性: 首先写出 $X_{n}$ 的分布
> $$\begin{align}
> f(t)=\binom{n}{1} \frac{1}{\theta}\times\left( \frac{t}{\theta} \right)^{n-1}=\frac{nt^{n-1}}{\theta^n}
> \end{align}$$
> 从而对任意 $g$ 满足 
> $$\mathbb E_{\theta}(g(T))=\frac{n}{\theta^n}\int_{0}^\theta t^{n-1}g(t)\ \mathrm{d}t=0,\quad \forall\theta$$
> 则 $g(t)\xlongequal{\text{a.s.}}0$ 这说明 $X_n$ 是完全的. 下面计算
> $$\mathbb E[X_{(n)}]=\frac{n}{\theta^n}\int_{0}^\theta t^n\ \mathrm{d}t=\frac{n\theta}{n+1}$$
> 所以给出 UMVUE 为 $\delta(X)=\frac{n+1}{n}X_{(n)}$
> 另一种找到这一估计的方法是, 首先拿出一个无偏估计 $\delta^*=2\bar{X}$ 再做修正
> $$\mathbb E[2\bar{X}\mid X_{(n)}]=2 \frac{ X_{(n)}+(n-1)\frac{X_{(n)}}{2} }{n}=\frac{n+1}{n}X_{(n)}$$
> 然而事实上, 假如我们不限于无偏估计, 实际上在所有形如 $cX_{(n)}$ 的估计中, $\frac{n+2}{n+1}X_{(n)}$ 在 MSE 上表现最好

> [!example]
> 考虑 Poission 分布族 $X_{1},\cdots,X_{n}\stackrel{\text{i.i.d}}{\sim}P(\theta)$ 其 p.m.f.
> $$p_{\theta }^{(1)}(x)=\frac{\theta^xe^{-\theta}}{x!},\quad x\in \mathbb N$$
> 则 $T(X)=\sum X_{i}\sim P(n\theta)$ 为一充分完全统计量, 其 p.m.f
> $$p_{\theta}^T(t)=\frac{(n\theta)^te^{-n\theta}}{t!}$$
> 尝试对 $\theta^2$ 进行无偏估计
> 首先尝试方法一: 拿 $\bar{X}^2$ 试一下, 由于 Jensen 不等式 $\mathbb E(\bar{X}^2)>\mathbb E(\bar{X})^2=\theta^2$, 行不通
> $$\begin{align}
> \delta(T)\ 无偏&\iff \sum\delta(t)p_{\theta}^T(t)=\theta^2&\forall\theta \\
&\iff \sum_{t=0}^\infty\delta(t)\frac{n^t\theta^t}{t!}=\theta^2e^{n\theta}=\sum_{k=0}^\infty\frac{n^k\theta^{k+2}}{k!}&\forall\theta
> \end{align}$$
>  从而有 $\delta(t)=\frac{n^{t-2}}{(t-2)!}\cdot\frac{t^2}{n^t}=\frac{t(t-1)}{n^2}$ 
>  方法二 (Rao-Blackwelliza): 首先随便找一个无偏估计 $\mathbb E_{\theta}[X_{1}X_{2}]=\theta^2$, 直接计算
>  $$\delta(T)=\mathbb E\left[ X_{1}X_{2}\mid \sum X_{i} \right]=\frac{T(T-1)}{n^2}$$

## 得分函数的微分恒等式和方差下界

假定 $\mathcal P$ 的分布关于 $\theta$ 具有共同的支撑集, 对数似然函数可以取 $l(\theta;x)=\log p_{\theta}(x)$, 此时定义**得分函数** (score function #xjb翻译 ) 为 $\nabla l(\theta,x)$, 此时由于对较小的 $\eta$ 有
$$p_{\theta+\eta}(x)=e^{l(\theta+\eta;x)}\approx p_{\theta}(x)e^{\eta^T\nabla l(\theta;x)}$$
这就是说, 局部上看, $\mathcal P$ 差不多是个指数族, 而 $\nabla l$ 差不多是个充分完全统计量, 而由于我们有
$$1=\int_{\mathcal X}e^{l(\theta,x)}\ \mathrm{d}\mu(x)\implies0= \frac{\partial}{\partial\theta_{j}}1=\int_{\mathcal X}\partial_{j}l(\theta;x)e^{l(\theta;x)}\ \mathrm{d}\mu(x)$$
这就是说
$$\mathbb E_{\theta}[\nabla l(\theta;X)]=0$$
对上边那个式子再微分一次就有
$$0=\int_{\mathcal X}\left( \frac{\partial^2l}{\partial\theta_{j}\partial\theta_{k}}+\frac{\partial l}{\partial\theta_{j}}\frac{\partial l}{\partial\theta_{k}} \right)e^{l(\theta;x)}=\mathbb E_{\theta}\left[  \frac{\partial^2}{\partial\theta_{j}\partial\theta_{k}}l(\theta;X) \right]+\mathbb E\left[ \frac{\partial}{\partial\theta_{j}}l(\theta;X)+ \frac{\partial}{\partial\theta_{k}}l(\theta;X) \right]$$
从而有
$$J(\theta):=\mathbb E_{\theta}[-\nabla^2l(\theta;X)]=var_{\theta}(\nabla l(\theta;X))$$
此处的 $J(\theta)$ 称为 $\theta$ 的 Fisher 信息量

把上面这一套再应用回估计中, 假定
$$g(\theta)=\mathbb E_{\theta}[\delta(X)]=\int_{\mathcal X}\delta(x)e^{l(\theta;x)}\ \mathrm{d}\mu(x)$$
则
$$\begin{align}
\nabla g(\theta)&=\int\delta \nabla le^l \\
&=\mathbb E_{\theta}[\delta(X)\nabla l(\theta;X)] \\
&=cov(\delta(X),\nabla l(\theta;X))
\end{align}$$
假定只有一个参数, 既 $\theta\in\mathbb R$, 则由 Cauchy-Schwarz ineq
$$var_{\theta}(\delta)var(l)\geq cov_{\theta}(\delta,l)^2$$
> [!thm] Cramer-Rao
> 对任意的 $g(\theta)$ 无偏估计 $\delta(X)$, 
> - $\forall x\in\mathcal X,\theta\in\Theta$, $\frac{\partial f(x\mid \theta)}{\partial\theta}$ 存在, 且在 $f$ 的支撑集上不恒为 $0$
> - $\frac{\partial}{\partial\theta}\int f(x\mid \theta)\ \mathrm{d}x=\int \frac{\partial}{\partial\theta}f(x\mid \theta)\ \mathrm{d}x$
> - $\frac{d}{d\theta}\mathbb E_{\theta}[W(X)]=\int \frac{\partial}{\partial\theta}[W(x)f(x\mid \theta)]\ dx$
> 若有 $\theta\in\mathbb R$ 则
> $$var_{\theta}(\delta)\geq \frac{g'(\theta)^2}{J(\theta)}$$
> 更普遍地, 对 $\theta\in\mathbb R^d$, $g(\theta)\in\mathbb R$
> $$var_{\theta}(\delta)\geq \nabla g(\theta)^TJ(\theta)^{-1}\nabla g(\theta)$$

事实上, 也可以先射箭后画靶子, 既指定 $g(\theta)=\mathbb E_{\theta}[\delta]$

但是在 Cramer-Rao 不等式中, 我们需要在积分号下求导, 若稍微放宽条件能否得到类似的结论呢, Hammersley-Chapman-Rabbins ineq 利用变差给出了一个更泛用的下界, 其核心想法是
$$\epsilon\ll 1\implies \frac{p_{\theta+\epsilon}(x)}{p_{\theta}(x)}-1\approx \epsilon^T\nabla l(\theta;x)$$
> [!thm] Hammersley-Chapman-Robbins
> 令 $\delta$ 为 $g(\theta)$ 的一个无偏估计, 假定对于某些 $\epsilon$ 有 $p_{\theta+ \epsilon}\ll p_{\theta}$ 则
> $$var_{\theta}(\delta)\geq \sup_{\epsilon}\frac{g(\theta+\epsilon)-g(\theta)}{\mathbb E_{\theta}\left[ \left(  \frac{p_{\theta+\epsilon}(X)}{p_{\theta}(X)}-1 \right)^2 \right]}$$

rmk: 
- 这里的 $\ll$ 不是远小于的意思, 而是连续性, 即 $p_{\theta}\to 0\implies p_{\theta+\epsilon}\to0$, 保证除法可行
- 此处若取 $\epsilon\to0$ 即得 Cramer-Rao 不等式, 但是取 sup 可以得到一个更好的上界

`BEGINPROOF`
注意到若有 $p_{\theta+\epsilon}\ll p_{\theta}$
$$\mathbb E_{\theta}\left[ \frac{p_{\theta+\epsilon}(x)}{p_{\theta}(x)} -1\right]=\int\left(  \frac{p_{\theta+\epsilon}}{p_{\theta}} -1 \right)p_{\theta}\ \mathrm{d}\mu=\int(p_{\theta+\epsilon}-p_{\theta})\ \mathrm{d}\mu=0$$
更进一步
$$\begin{align}
cov\left( \delta(X), \frac{p_{\theta+\epsilon}(X)}{p_{\theta}(X)}-1 \right)&=\int\delta p_{\theta+\epsilon}\ \mathrm{d}\mu-\int\delta p_{\theta}\ \mathrm{d}\mu \\
&=\mathbb E_{\theta+\epsilon}[\delta(X)]-\mathbb E_{\theta}[\delta(X)] \\
&=g(\theta+\epsilon)-g(\theta)
\end{align}$$
从而由 Cauchy-Schwarz 立得题式
`ENDPROOF`

## 估计效率

下一个问题是, Cramer-Rao 下界未必同时能取到, 我们将逼近这一下界的程度定义为估计的效率:
$$e_{\theta}(\delta)=\frac{\text{Cramer-Rao Lower Bound}}{var_{\theta}(\delta)}= \frac{g'(\theta)^2}{var_{\theta}(\delta)J(\theta)}\leq 1$$
可以发现另一个等价定义
$$e_{\theta}(\delta)=corr(\delta,l')^2=\frac{(\partial_{\theta}\mathbb E_{\theta}[\delta(X)])^2}{var_{\theta}(\delta)var(l'(\theta;X))}$$
#todo 没理解, corr 是笔误还是什么我不知道的函数呢

> [!example]
> 对于指数族
> $$p_{\theta}(x)=C(\theta)e^{\theta^TT(x)}h(x),\qquad l(\theta;x)=\theta^TT(x)+\log C(\theta)+\log h(x) $$
> 其评分函数
> $$\nabla l(\theta;x)=T(x)-\mathbb E_{\theta}[T(X)]$$
> (这里用了指数族的性质) 从而 Fisher 信息量为
> $$var_{\theta}(\nabla l(\theta;X))=var_{\theta}(T(X))=\nabla^2(-\log C(\theta))=\mathbb E_{\theta}[-\nabla^2l(\theta;X)]$$

# Bayes 估计

> [!example]
> 对二项分布 $X\sim B(n,\theta)$, 考虑两个估计量 $\delta_{0}(x)=\frac{x}{n}$ 和 $\delta_{1}(x)= \frac{x+3}{n+6}$ 后者倾向于估计值 $\frac{1}{2}$, 如何评价二者的估计精度?

过去我们只考虑无偏估计的好坏, 但我们可以直接优化 MSE 而不是局限于无偏估计, 事实上无偏估计有时候会给出一些很唐的结果

> [!example]
> 假定 $X\sim B(50,\theta)$, $g(\theta)=\mathbb P_{\theta}(X>25)$ 其 UMVUE 是 $\delta(X)=\mathbb 1_{\{X\geq25\}}$, 但是假如我们抽到了 $X=25$, 就会直接估计这一概率为 $1$
> 假定 $X\sim N_{d}(\theta,I_{d})$, 我们试图估计 $\|\theta\|_{2}^2$, 其 UMVUE 是 $\|X\|_{2}^2-d$, 但这个估计甚至能小于 $0$, 这显然不太合理

为解决这一问题, 我们提出 Bayes 估计, 假定给出损失函数 $L(\theta;\delta(X))$, 通常的风险函数为 $R(\theta;\delta)=\mathbb E_{\theta}[L(\theta;\delta(X))]$, 我们定义 Bayes 风险
$$R_{\text{Bayes}}(\Lambda;\delta)=\int_{\Omega}R(\theta;\delta)\ \mathrm{d}\Lambda(\theta)=\mathbb E[R(\Theta;\delta(X))]=\mathbb E[L(\Theta;\delta(X))]$$
这里 $\Theta\sim\Lambda$ 且 $X\mid \Theta=\theta\sim P_{\theta}$, 这里将 $R$ 依照某一给定的测度 $\Lambda$ 积分, 这里 $\Lambda$ 称为先验 (prior #xjb翻译 ), 我们暂且假定 $\Lambda(\Omega)=1$, 随后我们会允许 $\Lambda(\Omega)=\infty$, 称为不当先验

> [!def] Bayes 估计
> 若 $\delta(X)$ 最小化 $R_{\text{Bayes}}(\Lambda,\delta)$, 则称其为 Bayes 估计

事实上更加常用的定义是

> [!thm]
> $\Theta\sim\Lambda$, $X\mid _{\Theta=\theta}\sim P_{\theta}$, 若 $\forall\theta,d, L(\theta;d)\geq0$ 且 $\exists\delta_{0},R_{\text{Bayes}}(\Lambda;\delta_{0})<\infty$ 则
> $$\delta_\Lambda(x)\in \arg\min_{d}\mathbb E[L(\Theta;d)\mid X=x]\text{ for a.e.}x\iff \delta_{\Lambda}\text{ is Bayes}$$

`BEGINPROOF`
$\implies$: 对任意估计 $\delta$
$$\begin{align}
R_{\text{Bayes}}(\Lambda;\delta)&=\mathbb E[L(\Theta;\delta(X))] \\
&=\mathbb E[\mathbb E[L(\Theta;\delta(X))\mid X]] \\
&\geq \mathbb E[\mathbb E[L(\Theta;\delta_{\Lambda}(X))\mid X]] \\
&=R_{\text{Bayes}}(\Lambda;\delta_{\Lambda})
\end{align}$$
$\impliedby$: 反证, 设 $E_{x}(d):=\mathbb E[L(\Theta;d)\mid X=x]$, 取
$$\delta^*(x)=\begin{cases}
\delta_{\Lambda}(x)&\delta_{\Lambda}(x)\in\arg\min E_{x}(d) \\
\delta_{0}(x)&E_{x}(\delta_{0}(x))<E_{x}(\delta_\Lambda(x)) \\
d^*(x)&\text{otherwise} 
\end{cases}$$
其中 $d^*(x)$ 是使 $E_{x}(d^*(x))<E_{x}(\delta_{\Lambda}(x))$ 的点, 则
$$\begin{align}
E_{x}(\delta^*(X))&\leq_{\text{a.s.}} E_{x}(\delta_{0}(X)) \\
E_{x}(\delta^*(X))&\leq_{\text{a.s.}} E_{x}(\delta_{\Lambda}(X))
\end{align}$$
并且若后者在一个正测度集上取严格小于
$$R_{\text{Bayes}}(\Lambda,\delta^*)\leq R_{\text{Bayes}}(\delta_\Lambda(X))$$
产生矛盾
`ENDPROOF`

> [!proposition]
> 当我们取损失函数 $L(\theta;d)=(g(\theta)-d)^2$ 为平方误差, Bayes 估计即为
> $$\delta=\mathbb E[g(\Theta)\mid X]$$

`BEGINPROOF`
这是因为
$$R_{\text{Bayes}}(\Lambda,\delta)=\mathbb E[\mathbb E[(g(\Theta)-\delta(X))^2\mid X]]=\mathbb E[var(g(\Theta)\mid X)+(\mathbb E[g(\Theta)\mid X]-\delta(X))^2]$$
要最小化后面这个东西就必然要 $\delta(X)\xlongequal{\text{a.s.}}\mathbb E[g(\Theta)\mid X]$
`ENDPROOF`
> [!proposition]
> 若 $d\Lambda=\lambda dx$ 则称 $\lambda$ 为先验分布, 此时称 $\lambda(\theta\mid X)$ 为后验分布, $\delta_{\Lambda}(X)$ 最小化
> $$\mathbb E[L(\Theta;\delta_{\Lambda})\mid X=x]$$

## 共轭先验

> [!example] Beta-Binomial
> $X\sim B(n,\theta)=\theta^x(1-\theta)^{1-x}\binom nx$, 其先验分布 $\theta\sim Beta(\alpha,\beta)=\theta^{\alpha-1}(1-\theta)^{\beta-1}\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$
> 则后验分布
> $$\lambda(\theta\mid x)=\frac{p_{\theta}(x)\lambda(\theta)}{\int _{0}^1p_{v}(x)\lambda(v)\ \mathrm{d}v}\propto_{\theta}\theta^{x+\alpha-1}(1-\theta)^{\beta-x}\propto Beta(\alpha+x-1,\beta-x+1) $$
> 从而均方误差意义下
> $$\mathbb E[\Theta\mid X]= \frac{X+\alpha}{n+\alpha+\beta} $$

假如先验分布和后验分布来自同一分布族, 我们就称其为共轭先验 (显然这个定义并不很清晰, 其实就是个称呼而言), 一些常见的共轭先验

| 似然函数 | 先验分布(族) |
| ---- | ---- |
| 二项分布 | Beta 分布 |
| 正态分布 | 正态分布 |
| 泊松分布 | Gamma 分布 |


Bayes 估计要求我们凭空造出一个先验分布出来, 但问题是如何给出呢

- 直接先验, 平行检验: 我们可以从数据中获得先验, 若我们有对先验的广泛共识, 则更容易得到有意义的先验, 为此我们给出了以下两种 Bayes 估计
	- 多层 Bayes (Hierarchical Bayes)
	- 经验 Bayes (Empirical Bayes) #xjb翻译 
- 主观意愿: 统计的本质是大忽悠
- 找个好算的
- 客观先验: 假装这个先验不带主观判断

> [!example]
> 假定 $X_{i}\sim N(\theta,1)$, 应用平滑先验 $\lambda(\theta)\propto_{\theta}1$ (注意我们从来没要求先验是个概率分布), 我们仍然可以计算
> $$\lambda(\theta)p_{\theta}(x) \propto_{\theta} \exp\left\{ \theta \sum_{i}x_{i}-n\theta^2 / 2 \right\}\propto N\left( \bar{x}, \frac{1}{n} \right)$$
> 从而 Bayes 估计量为 $\bar{X}$, 

但是平滑先验有一个重要问题, 就是他在重参数化意义下不变

## 多层 Bayes

#todo 

### Markov Chain Monte Caelo, MCMC

> [!def] Markov 链
> 称一系列随机变量 $X^{(0)},X^{(1)},\cdots$ 是核为 $Q(y\mid x)$, 初始分布为 $\pi_{0}(x)$ 的 Markov 链, 是指其分布
> $$X^{(0)}\sim \pi_{0},\qquad X^{(t+1)}\mid X^{(0)},\cdots,X^{(t)}\sim Q(\cdot\mid X^{(t)})$$
> 换句话说, Markov 链上的元素每个影响下一个

对于一个分布 $\pi$, 我们称它是相对于核 $Q$ 静止 (stationarity), 如果
$$\pi(y)=\int_{\mathcal X}Q(y\mid x)\pi(x)\ \mathrm{d}\mu(x)$$
一个更强的条件是详细平衡(detailed balance #xjb翻译 )
$$\pi(x)Q(y\mid x)=\pi(y)Q(x\mid y)$$
只需对两侧关于 $x$ 积分即可由详细平衡推出静止

可以证明, 一个 Markov 链只要足够长, 最终都会走向其静止分布 #todo 

### Gibbs Sampler

#todo 

## 经验 Bayes

## 极大极小估计

对于一个统计模型 $\mathcal P=\{P_{\theta};\theta\in \Theta\}$, 我们由以下两种选择统计量的思路
- 限制统计量选择的范围 - 如无偏估计
- 最小化平均误差 - 如 Bayes 估计

首先介绍几个常用的风险函数: Bayes 风险, 极大极小风险, least favorable 风险:

在 Bayes 估计中, 我们选取先验 $\Lambda$ 满足 $\Lambda(\Theta)=1$, 再令 Bayes 估计最小化
$$R(\theta;\delta)=\mathbb E[L(\theta;\delta(X))]$$
此时我们定义问题 $\Lambda,\mathcal P$ 的 Bayes risk 为
$$r_{\Lambda}=\inf_{\delta}\int R(\theta,\delta)\ \mathrm{d}\Lambda(\theta)$$
所有的 Bayes 估计本质上都是在找这个 $\delta$

本节我们介绍极大极小估计, 核心想法是最小化
$$r^*=\min_{\delta}\sup_{\theta}R(\theta;\delta)$$
容易发现
$$r_{\Lambda}\leq r^*$$

另外, 如果我们能取到先验 $\Lambda^*$ 使得
$$r_{\Lambda^*}=\sup_{\Lambda}r_{\Lambda}$$
就称它为 least favorable (LF) 先验, 我们知道
$$\sup_{\theta}R(\theta;\delta)\geq r^*\geq r_{\Lambda^*}\geq r_{\Lambda}$$
我们希望找到一组先验 $\Lambda$ 和估计 $\delta$ 让上面那一排不等式变成等式

> [!thm]
> 若 $r_{\Lambda}=\sup_{\theta}R(\theta;\delta_{\Lambda})$ 其中 $\delta_{\Lambda}$ 是 $\Lambda$ 的 Bayes 估计, 则
> (a) $\delta_{\Lambda}$ 为极大极小估计
> (b) 若 $\delta_{\Lambda}$ 在 a.e. 全等意义下是唯一的 Bayes 估计, 则它同时也是唯一的极大极小估计
> (c) $\Lambda$ 是 LF 先验

`BEGINPROOF`
考虑对任意不同的估计 $\delta$
$$\sup_{\theta}R(\theta;\delta)\geq \int R(\theta;\delta)\ \mathrm{d}\Lambda(\theta)\geq \int R(\theta;\delta_{\Lambda})\ \mathrm{d}\Lambda(\theta)=r_{\Lambda}=\sup_{\theta} R(\theta;\delta_{\Lambda})$$
从而 (a) (b) 立得

(c) 对任意不同的先验 $\tilde{\Lambda}$
$$r_{\tilde{\Lambda}}=\inf_{\delta}\int R(\theta;\delta)\ \mathrm{d}\tilde{\Lambda}\leq \int R(\theta;\delta_{\Lambda})\ \mathrm{d}\Lambda\leq \sup_{\theta}R(\theta;\delta_{\Lambda})=r_{\Lambda}$$
`ENDPROOF`

> [!example]
> 假定二项分布 $X\sim B(n,\theta)$, $\theta\in[0,1]$, 我们试图用 MSE 估计 $\theta$, 采用先验分布 $\theta\sim Beta(\alpha,\beta)$, 我们已经计算过 Bayes 估计
> $$\delta_{\alpha,\beta}(X)= \frac{\alpha+X}{\alpha+\beta+n}$$
> 此时 Bayes 风险
> $$\begin{align}
> MSE(\theta;\delta_{\alpha,\beta})&=\mathbb E_{\theta}\left[ \left( \frac{\alpha+X}{\alpha+\beta+n}-\Theta \right)^2 \right] \\
> &\propto_{\theta}[(\alpha+\beta)^2-n]\theta^2+[n-2\alpha(\alpha+\beta)]\theta+\alpha^2
> \end{align}$$
> 为了使 $\delta$ 同时是极大极小估计, 我们希望选择合适的 $\alpha,\beta$ 使得 $MSE$ 相对于 $\theta$ 为常值, 即上述两个系数为 $0$, 解方程得 $\alpha=\beta= \frac{\sqrt{ n }}{2}$, 从而 $Beta\left( \frac{\sqrt{ n }}{2}, \frac{\sqrt{ n }}{2} \right)$ 为 LS 先验
> 但是实际上这个先验不太好用, 因为他在 $\frac{1}{2}$ 附近分配了过多的权重.

